{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment, random_py_environment, random_tf_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.networks import encoding_network\n",
    "\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network, network\n",
    "from tf_agents.networks.utils import BatchSquash\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer, episodic_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.utils import common as common_utils\n",
    "from tf_agents.utils import nest_utils\n",
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "import collections\n",
    "\n",
    "import gin\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.policies import boltzmann_policy\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.utils import composite\n",
    "from tf_agents.utils import eager_utils\n",
    "from tf_agents.utils import nest_utils\n",
    "from tf_agents.utils import training as training_lib\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MM1NQueue(object):\n",
    "    '''\n",
    "    define a M/M/1/N queue, where N = max queue size\n",
    "    '''\n",
    "    def __init__(self, ini_num_pkt, max_num_pkt, arrival_rate, service_rate):\n",
    "        self.ini_num_pkt = ini_num_pkt\n",
    "        self.max_num_pkt = max_num_pkt\n",
    "        self.arrival_rate = float(arrival_rate)\n",
    "        self.service_rate = float(service_rate)\n",
    "\n",
    "    def reset_ini_num_pkt(self, ini_num_pkt):\n",
    "        '''\n",
    "        reset the initial number of packets\n",
    "        '''\n",
    "        self.ini_num_pkt = ini_num_pkt\n",
    "\n",
    "    def run_one_unit_time(self):\n",
    "        '''\n",
    "        run the queue for one unit time and return the (queue_length, num_sevice) tuple\n",
    "        '''\n",
    "        queue_length = self.ini_num_pkt\n",
    "        time = 0\n",
    "        num_service = 0\n",
    "        while True:\n",
    "            t_arrival = np.random.exponential(scale=1/self.arrival_rate)\n",
    "            t_service = np.random.exponential(scale=1/self.service_rate)\n",
    "            if t_arrival > t_service:\n",
    "                time = time + t_service\n",
    "                if time > 1:\n",
    "                    break\n",
    "                if queue_length:\n",
    "                    num_service += 1\n",
    "                    queue_length -= 1\n",
    "            else:\n",
    "                time = time + t_arrival\n",
    "                if time > 1:\n",
    "                    break\n",
    "                queue_length = min(self.max_num_pkt, queue_length+1)\n",
    "        return queue_length, num_service\n",
    "\n",
    "    def run_multiple_unit_slots(self):\n",
    "        '''\n",
    "        invoke self.run_one_unit_time multiple times and obtain the following\n",
    "        statistics:\n",
    "        end_queue_length_frequency: a list with size self.max_num_pkt+1,\n",
    "                                    end_queue_length_frequency[n] is the frequency\n",
    "                                    of the queue length = n at the end of one unit time\n",
    "        average_num_service: the average number of services at the end of one unit time\n",
    "        '''\n",
    "        end_queue_length_frequency = np.array([0]*(self.max_num_pkt+1), dtype=float)\n",
    "        average_num_service = 0.0\n",
    "        num_runs = 100\n",
    "        for _ in range(num_runs):\n",
    "            (queue_length, num_service) = self.run_one_unit_time()\n",
    "            end_queue_length_frequency[queue_length] += 1\n",
    "            average_num_service += num_service\n",
    "        average_num_service = average_num_service/num_runs\n",
    "        end_queue_length_frequency = end_queue_length_frequency/float(num_runs)\n",
    "        return (end_queue_length_frequency, average_num_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarRentalEnv(py_environment.PyEnvironment):\n",
    "    def __init__(self, locations=2, max_cars=20, max_days=10, max_move=5):\n",
    "        self.locations = locations\n",
    "        self.max_cars = max_cars\n",
    "        self.max_days = max_days\n",
    "\n",
    "        self._action_spec = array_spec.BoundedArraySpec(shape=(),\n",
    "                                                        dtype=np.int32,\n",
    "                                                        minimum = 0,\n",
    "                                                        maximum = max_move,\n",
    "                                                        name='action')\n",
    "\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(shape=(locations,),\n",
    "                                                             dtype=np.int32,\n",
    "                                                             minimum=np.zeros(locations, dtype=np.int32),\n",
    "                                                             maximum=np.ones(locations, dtype=np.int32)*max_cars,\n",
    "                                                             name='observation')\n",
    "\n",
    "        self._state = np.zeros(locations, dtype=np.int32)\n",
    "        self._episode_ended = False\n",
    "        self._num_days = 0\n",
    "\n",
    "        self.location_queues = []\n",
    "        self.expected_rewards = []\n",
    "\n",
    "        self.location_queues.append(MM1NQueue(0, max_cars+1, 5, 4))\n",
    "        self.expected_rewards.append(np.zeros(shape=(self.max_cars + 1)))\n",
    "        \n",
    "        self.location_queues.append(MM1NQueue(0, max_cars+1, 1, 5))\n",
    "        self.expected_rewards.append(np.zeros(shape=(self.max_cars + 1)))\n",
    "\n",
    "        self.calc_all_rewards()\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state[0] = 10\n",
    "        self._state[1] = 10\n",
    "        self._episode_ended = False\n",
    "        self._num_days = 0\n",
    "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        self.move(action)\n",
    "        reward = self.get_daily_reward(action)\n",
    "\n",
    "        self._num_days += 1\n",
    "\n",
    "        if self.game_over():\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(np.array(self._state, dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(\n",
    "                np.array(self._state, dtype=np.int32), reward=reward, discount=0.9)\n",
    "\n",
    "    def calc_all_rewards(self):\n",
    "        '''\n",
    "        Reward of a morning state.\n",
    "        '''\n",
    "        for i, queue in enumerate(self.location_queues):\n",
    "            for ini_num_car in range(self.max_cars + 1):\n",
    "                queue.reset_ini_num_pkt(ini_num_car)\n",
    "                probs, reward = queue.run_multiple_unit_slots()\n",
    "                self.expected_rewards[i][ini_num_car] = reward\n",
    "\n",
    "    def get_daily_reward(self, action):\n",
    "        \"\"\"Gets the reward after a move has been made\"\"\"\n",
    "        total_reward = 0.\n",
    "        for loc in range(self.locations):\n",
    "            num_cars = int(self._state[loc])\n",
    "            total_reward += 10*self.expected_rewards[loc][num_cars] - 2*np.abs(action)\n",
    "        return total_reward\n",
    "\n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self._state[0] -= action\n",
    "        self._state[1] += action\n",
    "\n",
    "        self._state = np.clip(self._state, a_min = 0, a_max = self.max_cars)\n",
    "\n",
    "    def game_over(self):\n",
    "        return self._num_days > self.max_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = 2\n",
    "\n",
    "env1 = CarRentalEnv(locations)\n",
    "# utils.validate_py_environment(env, episodes=2)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(env1)\n",
    "\n",
    "env2 = CarRentalEnv(locations)\n",
    "env2.location_queues = env1.location_queues\n",
    "env2.expected_rewards = env1.expected_rewards\n",
    "\n",
    "# utils.validate_py_environment(env2, episodes=2)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  [[10 10]] Current reward: tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Action:  -3\n",
      "Current state:  [[13  7]] Current reward: tf.Tensor([80.6], shape=(1,), dtype=float32)\n",
      "Action:  -1\n",
      "Current state:  [[14  6]] Current reward: tf.Tensor([84.4], shape=(1,), dtype=float32)\n",
      "Action:  -5\n",
      "Current state:  [[19  1]] Current reward: tf.Tensor([36.4], shape=(1,), dtype=float32)\n",
      "Action:  -4\n",
      "Current state:  [[20  0]] Current reward: tf.Tensor([30.7], shape=(1,), dtype=float32)\n",
      "Action:  2\n",
      "Current state:  [[18  2]] Current reward: tf.Tensor([56.4], shape=(1,), dtype=float32)\n",
      "Action:  -2\n",
      "Current state:  [[20  0]] Current reward: tf.Tensor([38.7], shape=(1,), dtype=float32)\n",
      "Action:  1\n",
      "Current state:  [[19  1]] Current reward: tf.Tensor([52.4], shape=(1,), dtype=float32)\n",
      "Action:  -1\n",
      "Current state:  [[20  0]] Current reward: tf.Tensor([42.7], shape=(1,), dtype=float32)\n",
      "Action:  -4\n",
      "Current state:  [[20  0]] Current reward: tf.Tensor([30.7], shape=(1,), dtype=float32)\n",
      "Action:  0\n",
      "Current state:  [[20  0]] Current reward: tf.Tensor([46.7], shape=(1,), dtype=float32)\n",
      "Action:  3\n",
      "Current state:  [[17  3]] Current reward: tf.Tensor([63.9], shape=(1,), dtype=float32)\n",
      "Action:  4\n",
      "Current state:  [[10 10]] Current reward: tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Action:  -3\n",
      "Current state:  [[13  7]] Current reward: tf.Tensor([80.6], shape=(1,), dtype=float32)\n",
      "Action:  -4\n",
      "Current state:  [[17  3]] Current reward: tf.Tensor([59.9], shape=(1,), dtype=float32)\n",
      "Action:  2\n",
      "Current state:  [[15  5]] Current reward: tf.Tensor([78.3], shape=(1,), dtype=float32)\n",
      "Action:  -2\n",
      "Current state:  [[17  3]] Current reward: tf.Tensor([67.9], shape=(1,), dtype=float32)\n",
      "Action:  3\n",
      "Current state:  [[14  6]] Current reward: tf.Tensor([76.4], shape=(1,), dtype=float32)\n",
      "Action:  -3\n",
      "Current state:  [[17  3]] Current reward: tf.Tensor([63.9], shape=(1,), dtype=float32)\n",
      "Action:  -1\n",
      "Current state:  [[18  2]] Current reward: tf.Tensor([60.4], shape=(1,), dtype=float32)\n",
      "Action:  -1\n"
     ]
    }
   ],
   "source": [
    "time_step = train_env.reset()\n",
    "    \n",
    "for i in range(20):\n",
    "    print(\"Current state: \", time_step.observation.numpy(), \"Current reward:\", time_step.reward)\n",
    "    action = np.random.randint(-5, 5)\n",
    "    print(\"Action: \", action)\n",
    "    time_step = train_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 250 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 2 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 2000 # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "log_interval = 25 # @param {type:\"integer\"}\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 50 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "num_eval_episodes = 2\n",
    "#compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "  episode_counter = 0\n",
    "  environment.reset()\n",
    "\n",
    "  while episode_counter < num_episodes:\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "    if traj.is_boundary():\n",
    "      episode_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 25: loss = 1.1034411191940308\n",
      "step = 50: loss = 0.8009939193725586\n",
      "step = 50: Average Return = 903.5\n",
      "step = 75: loss = 2.451096534729004\n",
      "step = 100: loss = 2.5155601501464844\n",
      "step = 100: Average Return = 903.5\n",
      "step = 125: loss = 2.869663715362549\n",
      "step = 150: loss = -1.765895962715149\n",
      "step = 150: Average Return = 903.5\n",
      "step = 175: loss = 3.6954517364501953\n",
      "step = 200: loss = 0.48894238471984863\n",
      "step = 200: Average Return = 903.5\n",
      "step = 225: loss = 1.983219027519226\n",
      "step = 250: loss = 0.5128098726272583\n",
      "step = 250: Average Return = 903.5\n"
     ]
    }
   ],
   "source": [
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "  collect_episode(\n",
    "      train_env, tf_agent.collect_policy, collect_episodes_per_iteration)\n",
    "\n",
    "  # Use data from the buffer and update the agent's network.\n",
    "  experience = replay_buffer.gather_all()\n",
    "  train_loss = tf_agent.train(experience)\n",
    "  replay_buffer.clear()\n",
    "\n",
    "  step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb0438ff790>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[10 10]], shape=(1, 2), dtype=int32)\n",
      "State: tf.Tensor([[10 10]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[ 9 11]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[ 8 12]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[ 7 13]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[ 6 14]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[ 5 15]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[ 4 16]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[ 3 17]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[ 2 18]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[ 1 19]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "time_step = train_env.reset()\n",
    "print(time_step.observation)\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"State:\", time_step.observation)\n",
    "    action_step = tf_agent.policy.action(time_step)\n",
    "    print(\"Action:\", action_step.action)\n",
    "    time_step = train_env.step(action_step.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
