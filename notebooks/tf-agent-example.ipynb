{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment, random_py_environment, random_tf_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.networks import encoding_network\n",
    "\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network, network\n",
    "from tf_agents.networks.utils import BatchSquash\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.utils import common as common_utils\n",
    "from tf_agents.utils import nest_utils\n",
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "import collections\n",
    "\n",
    "import gin\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.policies import boltzmann_policy\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.utils import composite\n",
    "from tf_agents.utils import eager_utils\n",
    "from tf_agents.utils import nest_utils\n",
    "from tf_agents.utils import training as training_lib\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MM1NQueue(object):\n",
    "    '''\n",
    "    define a M/M/1/N queue, where N = max queue size\n",
    "    '''\n",
    "    def __init__(self, ini_num_pkt, max_num_pkt, arrival_rate, service_rate):\n",
    "        self.ini_num_pkt = ini_num_pkt\n",
    "        self.max_num_pkt = max_num_pkt\n",
    "        self.arrival_rate = float(arrival_rate)\n",
    "        self.service_rate = float(service_rate)\n",
    "\n",
    "    def reset_ini_num_pkt(self, ini_num_pkt):\n",
    "        '''\n",
    "        reset the initial number of packets\n",
    "        '''\n",
    "        self.ini_num_pkt = ini_num_pkt\n",
    "\n",
    "    def run_one_unit_time(self):\n",
    "        '''\n",
    "        run the queue for one unit time and return the (queue_length, num_sevice) tuple\n",
    "        '''\n",
    "        queue_length = self.ini_num_pkt\n",
    "        time = 0\n",
    "        num_service = 0\n",
    "        while True:\n",
    "            t_arrival = np.random.exponential(scale=1/self.arrival_rate)\n",
    "            t_service = np.random.exponential(scale=1/self.service_rate)\n",
    "            if t_arrival > t_service:\n",
    "                time = time + t_service\n",
    "                if time > 1:\n",
    "                    break\n",
    "                if queue_length:\n",
    "                    num_service += 1\n",
    "                    queue_length -= 1\n",
    "            else:\n",
    "                time = time + t_arrival\n",
    "                if time > 1:\n",
    "                    break\n",
    "                queue_length = min(self.max_num_pkt, queue_length+1)\n",
    "        return queue_length, num_service\n",
    "\n",
    "    def run_multiple_unit_slots(self):\n",
    "        '''\n",
    "        invoke self.run_one_unit_time multiple times and obtain the following\n",
    "        statistics:\n",
    "        end_queue_length_frequency: a list with size self.max_num_pkt+1,\n",
    "                                    end_queue_length_frequency[n] is the frequency\n",
    "                                    of the queue length = n at the end of one unit time\n",
    "        average_num_service: the average number of services at the end of one unit time\n",
    "        '''\n",
    "        end_queue_length_frequency = np.array([0]*(self.max_num_pkt+1), dtype=float)\n",
    "        average_num_service = 0.0\n",
    "        num_runs = 10000\n",
    "        for _ in range(num_runs):\n",
    "            (queue_length, num_service) = self.run_one_unit_time()\n",
    "            end_queue_length_frequency[queue_length] += 1\n",
    "            average_num_service += num_service\n",
    "        average_num_service = average_num_service/num_runs\n",
    "        end_queue_length_frequency = end_queue_length_frequency/float(num_runs)\n",
    "        return (end_queue_length_frequency, average_num_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarRentalEnv(py_environment.PyEnvironment):\n",
    "    def __init__(self, locations=2, max_cars=10, max_days=100):\n",
    "        self.locations = locations\n",
    "        self.max_cars = max_cars\n",
    "        self.max_days = max_days\n",
    "\n",
    "        self._action_spec = array_spec.BoundedArraySpec(shape=(1, locations),\n",
    "                                                        dtype=np.float32,\n",
    "                                                        minimum = -np.ones(locations, dtype=np.float32)*max_cars,\n",
    "                                                        maximum = np.ones(locations, dtype=np.float32)*max_cars,\n",
    "                                                        name='action')\n",
    "\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(shape=(locations,),\n",
    "                                                             dtype=np.float32,\n",
    "                                                             minimum=np.zeros(locations, dtype=np.float32),\n",
    "                                                             maximum=np.ones(locations, dtype=np.float32)*max_cars,\n",
    "                                                             name='observation')\n",
    "\n",
    "        self._state = np.zeros(locations, dtype=np.float32)\n",
    "        self._episode_ended = False\n",
    "        self._num_days = 0\n",
    "        self.max_days = 10\n",
    "\n",
    "        self.location_queues = []\n",
    "        self.expected_rewards = []\n",
    "\n",
    "        for i in range(locations):\n",
    "            self.location_queues.append(MM1NQueue(0, max_cars+1, np.random.randint(1, 5),\n",
    "                                             np.random.randint(1, 5)))\n",
    "            self.expected_rewards.append(np.zeros(shape=(self.max_cars + 1)))\n",
    "\n",
    "        self.calc_all_rewards()\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.zeros(self.locations, dtype=np.float32)\n",
    "        self._episode_ended = False\n",
    "        self._num_days = 0\n",
    "        return ts.restart(np.array(self._state, dtype=np.float32))        \n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        self.move(action)\n",
    "        reward = self.get_daily_reward()\n",
    "\n",
    "        self._num_days += 1\n",
    "\n",
    "        if self.game_over():\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(np.array(self._state, dtype=np.float32), reward)\n",
    "        else:\n",
    "            return ts.transition(\n",
    "                np.array(self._state, dtype=np.float32), reward=reward, discount=0.9)\n",
    "\n",
    "    def calc_all_rewards(self):\n",
    "        '''\n",
    "        Reward of a morning state.\n",
    "        '''\n",
    "        for i, queue in enumerate(self.location_queues):\n",
    "            for ini_num_car in range(self.max_cars + 1):\n",
    "                queue.reset_ini_num_pkt(ini_num_car)\n",
    "                probs, reward = queue.run_multiple_unit_slots()\n",
    "                self.expected_rewards[i][ini_num_car] = 10*reward\n",
    "\n",
    "    def get_daily_reward(self):\n",
    "        \"\"\"Gets the reward after a move has been made\"\"\"\n",
    "        total_reward = 0.\n",
    "        for loc in range(self.locations):\n",
    "            num_cars = int(self._state[loc])\n",
    "            total_reward += self.expected_rewards[loc][num_cars]\n",
    "        return total_reward\n",
    "\n",
    "    def move(self, action):\n",
    "        for location, a in enumerate(action[0]):\n",
    "            # Add only till max\n",
    "            if ((self._state[location] + a >= 0) and (self._state[location] + a < 5)):\n",
    "                self._state[location] += a\n",
    "\n",
    "    def game_over(self):\n",
    "        return self._num_days > self.max_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = 4\n",
    "\n",
    "env = CarRentalEnv(locations)\n",
    "utils.validate_py_environment(env, episodes=2)\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4.25315869 5.30459643 0.56569638 4.59061214]]]\n",
      "tf.Tensor([[0. 0. 0. 0.]], shape=(1, 4), dtype=float32) tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([[4.2531586  0.         0.56569636 4.590612  ]], shape=(1, 4), dtype=float32) tf.Tensor([56.973], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "action = np.random.uniform(0, 10, size=(1, 1, locations))\n",
    "print(action)\n",
    "time_step = tf_env.step(action.astype(np.float32))\n",
    "print(time_step.observation, time_step.reward)\n",
    "time_step = tf_env.step(action.astype(np.float32))\n",
    "print(time_step.observation, time_step.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([0., 0., 0., 0.], dtype=float32), maximum=array([10., 10., 10., 10.], dtype=float32))) BoundedTensorSpec(shape=(1, 4), dtype=tf.float32, name='action', minimum=array([-10., -10., -10., -10.], dtype=float32), maximum=array([10., 10., 10., 10.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(tf_env.time_step_spec(), tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = 2\n",
    "\n",
    "env = CarRentalEnv(locations)\n",
    "utils.validate_py_environment(env, episodes=2)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "env = CarRentalEnv(locations)\n",
    "utils.validate_py_environment(env, episodes=2)\n",
    "\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_capacity = 1000\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@test {\"skip\": true}\n",
    "\n",
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "  episode_counter = 0\n",
    "  environment.reset()\n",
    "\n",
    "  while episode_counter < num_episodes:\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "    if traj.is_boundary():\n",
    "      episode_counter += 1\n",
    "    \n",
    "    \n",
    "\n",
    "#@test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 2\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[244.94]\n"
     ]
    }
   ],
   "source": [
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 25: loss = 0.5358446836471558\n",
      "step = 50: loss = 1.4947469234466553\n",
      "step = 50: Average Return = 284.2070007324219\n",
      "step = 75: loss = 0.5565844774246216\n",
      "step = 100: loss = -0.519690990447998\n",
      "step = 100: Average Return = 306.0129699707031\n",
      "step = 125: loss = -0.8067162036895752\n",
      "step = 150: loss = -7.479184627532959\n",
      "step = 150: Average Return = 341.1029968261719\n",
      "step = 175: loss = -0.20073860883712769\n",
      "step = 200: loss = 1.690175175666809\n",
      "step = 200: Average Return = 338.0150451660156\n",
      "step = 225: loss = 0.569706380367279\n",
      "step = 250: loss = -1.1815063953399658\n",
      "step = 250: Average Return = 351.947021484375\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 250 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 2 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 2000 # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (10,10)\n",
    "\n",
    "\n",
    "log_interval = 25 # @param {type:\"integer\"}\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 50 # @param {type:\"integer\"}\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "  collect_episode(\n",
    "      train_env, tf_agent.collect_policy, collect_episodes_per_iteration)\n",
    "\n",
    "  # Use data from the buffer and update the agent's network.\n",
    "  experience = replay_buffer.gather_all()\n",
    "  train_loss = tf_agent.train(experience)\n",
    "  replay_buffer.clear()\n",
    "\n",
    "  step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
