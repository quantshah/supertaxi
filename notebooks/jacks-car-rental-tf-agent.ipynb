{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment, random_py_environment, random_tf_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.networks import encoding_network\n",
    "\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network, network\n",
    "from tf_agents.networks.utils import BatchSquash\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer, episodic_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.utils import common as common_utils\n",
    "from tf_agents.utils import nest_utils\n",
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "import collections\n",
    "\n",
    "import gin\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.policies import boltzmann_policy\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.utils import composite\n",
    "from tf_agents.utils import eager_utils\n",
    "from tf_agents.utils import nest_utils\n",
    "from tf_agents.utils import training as training_lib\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarRentalEnv(py_environment.PyEnvironment):\n",
    "    def __init__(self, locations=2, max_cars=15, max_days=100, max_move=5):\n",
    "        self.locations = locations\n",
    "        self.max_cars = max_cars\n",
    "        self.max_days = max_days\n",
    "\n",
    "        self._action_spec = array_spec.BoundedArraySpec(shape=(),\n",
    "                                                        dtype=np.int32,\n",
    "                                                        minimum = 0,\n",
    "                                                        maximum = 10,\n",
    "                                                        name='action')\n",
    "\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(shape=(locations,),\n",
    "                                                             dtype=np.int32,\n",
    "                                                             minimum=np.zeros(locations, dtype=np.int32),\n",
    "                                                             maximum=np.ones(locations, dtype=np.int32)*max_cars,\n",
    "                                                             name='observation')\n",
    "\n",
    "        self._state = np.zeros(locations, dtype=np.int32)\n",
    "        self._episode_ended = False\n",
    "        self._num_days = 0\n",
    "        \n",
    "        self.total_rented = [0, 0]\n",
    "        self.rental_rate = [3, 4]\n",
    "        self.return_rate = [3, 2]\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state[0] = np.random.randint(0, self.max_cars)\n",
    "        self._state[1] = self.max_cars - self._state[0]\n",
    "        self.total_rented = [0, 0]\n",
    "        self._episode_ended = False\n",
    "        self._num_days = 0\n",
    "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        reward = self.move(action)\n",
    "\n",
    "        self._num_days += 1\n",
    "\n",
    "        if self.game_over():\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(np.array(self._state, dtype=np.int32), reward)\n",
    "        else:\n",
    "            return ts.transition(\n",
    "                np.array(self._state, dtype=np.int32), reward=reward, discount=0.9)\n",
    "\n",
    "    def calc_all_rewards(self):\n",
    "        '''\n",
    "        Reward of a morning state.\n",
    "        '''\n",
    "        for i, queue in enumerate(self.location_queues):\n",
    "            for ini_num_car in range(self.max_cars + 1):\n",
    "                queue.reset_ini_num_pkt(ini_num_car)\n",
    "                probs, reward = queue.run_multiple_unit_slots()\n",
    "                self.expected_rewards[i][ini_num_car] = reward\n",
    "\n",
    "\n",
    "    def total_cars(self, start_cars, total_rented, rental_rate, return_rate):\n",
    "        rented = np.random.poisson(rental_rate)\n",
    "\n",
    "        if rented > start_cars:\n",
    "            rented = start_cars\n",
    "        else:\n",
    "            pass\n",
    "        total_rented += rented\n",
    "\n",
    "        returned = np.random.poisson(return_rate)\n",
    "\n",
    "        if returned > total_rented:\n",
    "            returned = total_rented\n",
    "\n",
    "        total_rented -= returned\n",
    "        final_cars = start_cars - rented + returned\n",
    "        return final_cars, rented, total_rented\n",
    "\n",
    "    \n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Update the state at end of the day\n",
    "        reward = 0\n",
    "        for i in range(self.locations):\n",
    "            self._state[i], rented, self.total_rented[i] = self.total_cars(self._state[i],\n",
    "                                                                           self.total_rented[i],\n",
    "                                                                           self.rental_rate[i],\n",
    "                                                                           self.return_rate[i])\n",
    "            reward += rented*10\n",
    "\n",
    "        if action>5:\n",
    "            self._state[0] += action-5\n",
    "            self._state[1] -= action-5\n",
    "            reward -= 2*(action-5)\n",
    "        else:\n",
    "            self._state[0] -= action\n",
    "            self._state[1] += action\n",
    "            reward -= 2*(action)\n",
    "        self._state = np.clip(self._state, a_min = 0, a_max = self.max_cars)\n",
    "        return reward\n",
    "\n",
    "    def game_over(self):\n",
    "        return self._num_days > self.max_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = 2\n",
    "\n",
    "env1 = CarRentalEnv(locations)\n",
    "utils.validate_py_environment(env1, episodes=2)\n",
    "train_env = tf_py_environment.TFPyEnvironment(env1)\n",
    "\n",
    "env2 = CarRentalEnv(locations)\n",
    "utils.validate_py_environment(env2, episodes=2)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:  [[ 1 14]] Current reward: tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Action:  5\n",
      "Current state:  [[ 0 15]] Current reward: tf.Tensor([50.], shape=(1,), dtype=float32)\n",
      "Action:  8\n",
      "Current state:  [[3 8]] Current reward: tf.Tensor([44.], shape=(1,), dtype=float32)\n",
      "Action:  8\n",
      "Current state:  [[4 2]] Current reward: tf.Tensor([74.], shape=(1,), dtype=float32)\n",
      "Action:  3\n",
      "Current state:  [[1 4]] Current reward: tf.Tensor([14.], shape=(1,), dtype=float32)\n",
      "Action:  3\n",
      "Current state:  [[0 8]] Current reward: tf.Tensor([24.], shape=(1,), dtype=float32)\n",
      "Action:  10\n",
      "Current state:  [[5 1]] Current reward: tf.Tensor([50.], shape=(1,), dtype=float32)\n",
      "Action:  7\n",
      "Current state:  [[4 0]] Current reward: tf.Tensor([46.], shape=(1,), dtype=float32)\n",
      "Action:  0\n",
      "Current state:  [[3 1]] Current reward: tf.Tensor([20.], shape=(1,), dtype=float32)\n",
      "Action:  10\n",
      "Current state:  [[12  0]] Current reward: tf.Tensor([0.], shape=(1,), dtype=float32)\n",
      "Action:  3\n",
      "Current state:  [[9 4]] Current reward: tf.Tensor([14.], shape=(1,), dtype=float32)\n",
      "Action:  6\n",
      "Current state:  [[10  4]] Current reward: tf.Tensor([28.], shape=(1,), dtype=float32)\n",
      "Action:  8\n",
      "Current state:  [[13  3]] Current reward: tf.Tensor([54.], shape=(1,), dtype=float32)\n",
      "Action:  5\n",
      "Current state:  [[8 7]] Current reward: tf.Tensor([30.], shape=(1,), dtype=float32)\n",
      "Action:  4\n",
      "Current state:  [[2 9]] Current reward: tf.Tensor([62.], shape=(1,), dtype=float32)\n",
      "Action:  8\n",
      "Current state:  [[6 8]] Current reward: tf.Tensor([44.], shape=(1,), dtype=float32)\n",
      "Action:  4\n",
      "Current state:  [[ 3 10]] Current reward: tf.Tensor([42.], shape=(1,), dtype=float32)\n",
      "Action:  4\n",
      "Current state:  [[ 0 15]] Current reward: tf.Tensor([42.], shape=(1,), dtype=float32)\n",
      "Action:  6\n",
      "Current state:  [[1 9]] Current reward: tf.Tensor([68.], shape=(1,), dtype=float32)\n",
      "Action:  4\n",
      "Current state:  [[ 0 15]] Current reward: tf.Tensor([32.], shape=(1,), dtype=float32)\n",
      "Action:  4\n"
     ]
    }
   ],
   "source": [
    "time_step = train_env.reset()\n",
    "    \n",
    "for i in range(20):\n",
    "    print(\"Current state: \", time_step.observation.numpy(), \"Current reward:\", time_step.reward)\n",
    "    action = np.random.randint(0, 11)\n",
    "    print(\"Action: \", action)\n",
    "    time_step = train_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 10  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 128  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "fc_layer_params = (20, 20)\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4152.4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(128, 2), observation=(128, 2, 2), action=(128, 2), policy_info=(), next_step_type=(128, 2), reward=(128, 2), discount=(128, 2)), BufferInfo(ids=(128, 2), probabilities=(128,))), types: (Trajectory(step_type=tf.int32, observation=tf.int32, action=tf.int32, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fded927cf90>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 1030.2835693359375\n",
      "step = 400: loss = 686.690673828125\n",
      "step = 600: loss = 2790.5126953125\n",
      "step = 800: loss = 1553.9183349609375\n",
      "step = 1000: loss = 1926.68994140625\n",
      "step = 1000: Average Return = 3189.199951171875\n",
      "step = 1200: loss = 1126.546630859375\n",
      "step = 1400: loss = 878.4148559570312\n",
      "step = 1600: loss = 1978.81298828125\n",
      "step = 1800: loss = 1672.1680908203125\n",
      "step = 2000: loss = 1538.7518310546875\n",
      "step = 2000: Average Return = 3425.60009765625\n",
      "step = 2200: loss = 1699.5654296875\n",
      "step = 2400: loss = 2414.43603515625\n",
      "step = 2600: loss = 2578.261962890625\n",
      "step = 2800: loss = 642.92138671875\n",
      "step = 3000: loss = 1787.59033203125\n",
      "step = 3000: Average Return = 3046.199951171875\n",
      "step = 3200: loss = 1911.8980712890625\n",
      "step = 3400: loss = 979.294677734375\n",
      "step = 3600: loss = 469.83892822265625\n",
      "step = 3800: loss = 1761.9146728515625\n",
      "step = 4000: loss = 944.4429931640625\n",
      "step = 4000: Average Return = 3433.0\n",
      "step = 4200: loss = 946.4193725585938\n",
      "step = 4400: loss = 287.17926025390625\n",
      "step = 4600: loss = 867.5010375976562\n",
      "step = 4800: loss = 2308.59619140625\n",
      "step = 5000: loss = 4031.8095703125\n",
      "step = 5000: Average Return = 3825.60009765625\n",
      "step = 5200: loss = 1060.17041015625\n",
      "step = 5400: loss = 227.03445434570312\n",
      "step = 5600: loss = 1330.9605712890625\n",
      "step = 5800: loss = 211.65843200683594\n",
      "step = 6000: loss = 1058.17333984375\n",
      "step = 6000: Average Return = 3810.39990234375\n",
      "step = 6200: loss = 1189.3779296875\n",
      "step = 6400: loss = 994.6527099609375\n",
      "step = 6600: loss = 2086.8876953125\n",
      "step = 6800: loss = 3972.8046875\n",
      "step = 7000: loss = 337.0586853027344\n",
      "step = 7000: Average Return = 3996.39990234375\n",
      "step = 7200: loss = 246.73236083984375\n",
      "step = 7400: loss = 1078.0072021484375\n",
      "step = 7600: loss = 3642.456787109375\n",
      "step = 7800: loss = 1928.9642333984375\n",
      "step = 8000: loss = 234.1795196533203\n",
      "step = 8000: Average Return = 4482.60009765625\n",
      "step = 8200: loss = 181.83480834960938\n",
      "step = 8400: loss = 256.16925048828125\n",
      "step = 8600: loss = 2394.99267578125\n",
      "step = 8800: loss = 253.43212890625\n",
      "step = 9000: loss = 3131.77783203125\n",
      "step = 9000: Average Return = 4438.2001953125\n",
      "step = 9200: loss = 227.0318603515625\n",
      "step = 9400: loss = 1006.6611938476562\n",
      "step = 9600: loss = 216.09092712402344\n",
      "step = 9800: loss = 223.0945587158203\n",
      "step = 10000: loss = 1172.6688232421875\n",
      "step = 10000: Average Return = 4065.39990234375\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fded927ca10>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[9 6]], shape=(1, 2), dtype=int32)\n",
      "State: tf.Tensor([[9 6]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[5 1]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[4 2]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[6 0]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[3 1]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[7 1]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[6 1]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[5 2]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[5 0]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "State: tf.Tensor([[4 3]], shape=(1, 2), dtype=int32)\n",
      "Action: tf.Tensor([0], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "time_step = train_env.reset()\n",
    "print(time_step.observation)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print(\"State:\", time_step.observation)\n",
    "    action_step = agent.policy.action(time_step)\n",
    "    print(\"Action:\", action_step.action)\n",
    "    time_step = train_env.step(action_step.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[4, 5]], dtype=int32)>,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "time_step = train_env.reset()\n",
    "max_num_car = 15\n",
    "\n",
    "policy_matrix = np.zeros((max_num_car+1, max_num_car+1))\n",
    "\n",
    "for i, j in product(range(max_num_car+1), range(max_num_car+1)):\n",
    "    state = tf.constant(np.array([[i, j]]), dtype=tf.int32)\n",
    "    policy_output = agent.policy.action(ts.TimeStep(time_step.step_type, reward=time_step.reward,\n",
    "                                 discount=time_step.discount,\n",
    "                                 observation=state))\n",
    "    action = policy_output.action.numpy()[0]\n",
    "    \n",
    "    if action > 5:\n",
    "        action -= 5\n",
    "    \n",
    "    policy_matrix[i,j] = action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fdec811a4d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = plt.matshow(policy_matrix)\n",
    "plt.colorbar(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
